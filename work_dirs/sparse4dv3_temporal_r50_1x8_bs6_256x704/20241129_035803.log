2024-11-29 03:58:04,009 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
CUDA available: True
GPU 0,1,2,3: NVIDIA L20
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.7, V11.7.99
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.13.0+cu117
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0+cu117
OpenCV: 4.10.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.7
MMDetection: 2.28.2+c41df4b
------------------------------------------------------------

2024-11-29 03:58:08,145 - mmdet - INFO - Distributed training: False
2024-11-29 03:58:12,242 - mmdet - INFO - Config:
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/sparse4dv3_temporal_r50_1x8_bs6_256x704'
total_batch_size = 48
num_gpus = 8
batch_size = 6
num_iters_per_epoch = 586
num_epochs = 100
checkpoint_epoch_interval = 20
checkpoint_config = dict(interval=11720)
log_config = dict(
    interval=51,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False),
        dict(type='TensorboardLoggerHook')
    ])
load_from = None
resume_from = None
workflow = [('train', 1)]
fp16 = dict(loss_scale=32.0)
input_shape = (704, 256)
tracking_test = True
tracking_threshold = 0.2
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
num_classes = 10
embed_dims = 256
num_groups = 8
num_decoder = 6
num_single_frame_decoder = 1
use_deformable_func = True
strides = [4, 8, 16, 32]
num_levels = 4
num_depth_layers = 3
drop_out = 0.1
temporal = True
decouple_attn = True
with_quality_estimation = True
model = dict(
    type='Sparse4D',
    use_grid_mask=True,
    use_deformable_func=True,
    img_backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        frozen_stages=-1,
        norm_eval=False,
        style='pytorch',
        with_cp=True,
        out_indices=(0, 1, 2, 3),
        norm_cfg=dict(type='BN', requires_grad=True),
        pretrained='ckpt/resnet50-19c8e357.pth'),
    img_neck=dict(
        type='FPN',
        num_outs=4,
        start_level=0,
        out_channels=256,
        add_extra_convs='on_output',
        relu_before_extra_convs=True,
        in_channels=[256, 512, 1024, 2048]),
    depth_branch=dict(
        type='DenseDepthNet',
        embed_dims=256,
        num_depth_layers=3,
        loss_weight=0.2),
    head=dict(
        type='Sparse4DHead',
        cls_threshold_to_reg=0.05,
        decouple_attn=True,
        instance_bank=dict(
            type='InstanceBank',
            num_anchor=900,
            embed_dims=256,
            anchor='nuscenes_kmeans900.npy',
            anchor_handler=dict(type='SparseBox3DKeyPointsGenerator'),
            num_temp_instances=600,
            confidence_decay=0.6,
            feat_grad=False),
        anchor_encoder=dict(
            type='SparseBox3DEncoder',
            vel_dims=3,
            embed_dims=[128, 32, 32, 64],
            mode='cat',
            output_fc=False,
            in_loops=1,
            out_loops=4),
        num_single_frame_decoder=1,
        operation_order=[
            'deformable', 'ffn', 'norm', 'refine', 'temp_gnn', 'gnn', 'norm',
            'deformable', 'ffn', 'norm', 'refine', 'temp_gnn', 'gnn', 'norm',
            'deformable', 'ffn', 'norm', 'refine', 'temp_gnn', 'gnn', 'norm',
            'deformable', 'ffn', 'norm', 'refine', 'temp_gnn', 'gnn', 'norm',
            'deformable', 'ffn', 'norm', 'refine', 'temp_gnn', 'gnn', 'norm',
            'deformable', 'ffn', 'norm', 'refine'
        ],
        temp_graph_model=dict(
            type='MultiheadAttention',
            embed_dims=512,
            num_heads=8,
            batch_first=True,
            dropout=0.1),
        graph_model=dict(
            type='MultiheadAttention',
            embed_dims=512,
            num_heads=8,
            batch_first=True,
            dropout=0.1),
        norm_layer=dict(type='LN', normalized_shape=256),
        ffn=dict(
            type='AsymmetricFFN',
            in_channels=512,
            pre_norm=dict(type='LN'),
            embed_dims=256,
            feedforward_channels=1024,
            num_fcs=2,
            ffn_drop=0.1,
            act_cfg=dict(type='ReLU', inplace=True)),
        deformable_model=dict(
            type='DeformableFeatureAggregation',
            embed_dims=256,
            num_groups=8,
            num_levels=4,
            num_cams=6,
            attn_drop=0.15,
            use_deformable_func=True,
            use_camera_embed=True,
            residual_mode='cat',
            kps_generator=dict(
                type='SparseBox3DKeyPointsGenerator',
                num_learnable_pts=6,
                fix_scale=[[0, 0, 0], [0.45, 0, 0], [-0.45, 0, 0],
                           [0, 0.45, 0], [0, -0.45, 0], [0, 0, 0.45],
                           [0, 0, -0.45]])),
        refine_layer=dict(
            type='SparseBox3DRefinementModule',
            embed_dims=256,
            num_cls=10,
            refine_yaw=True,
            with_quality_estimation=True),
        sampler=dict(
            type='SparseBox3DTarget',
            num_dn_groups=5,
            num_temp_dn_groups=3,
            dn_noise_scale=[2.0, 2.0, 2.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],
            max_dn_gt=32,
            add_neg_dn=True,
            cls_weight=2.0,
            box_weight=0.25,
            reg_weights=[2.0, 2.0, 2.0, 0.5, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0],
            cls_wise_reg_weights=dict(
                {9: [2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]})),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_reg=dict(
            type='SparseBox3DLoss',
            loss_box=dict(type='L1Loss', loss_weight=0.25),
            loss_centerness=dict(type='CrossEntropyLoss', use_sigmoid=True),
            loss_yawness=dict(type='GaussianFocalLoss'),
            cls_allow_reverse=[5]),
        decoder=dict(type='SparseBox3DDecoder'),
        reg_weights=[2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]))
dataset_type = 'NuScenes3DDetTrackDataset'
data_root = 'data/nuscenes/'
anno_root = 'data/nuscenes_anno_pkls/'
file_client_args = dict(backend='disk')
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(type='ResizeCropFlipImage'),
    dict(type='MultiScaleDepthMapGenerator', downsample=[4, 8, 16]),
    dict(type='BBoxRotation'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(
        type='CircleObjectRangeFilter',
        class_dist_thred=[55, 55, 55, 55, 55, 55, 55, 55, 55, 55]),
    dict(
        type='InstanceNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(type='NuScenesSparse4DAdaptor'),
    dict(
        type='Collect',
        keys=[
            'img', 'timestamp', 'projection_mat', 'image_wh', 'gt_depth',
            'focal', 'gt_bboxes_3d', 'gt_labels_3d'
        ],
        meta_keys=['T_global', 'T_global_inv', 'timestamp', 'instance_id'])
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(type='ResizeCropFlipImage'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='NuScenesSparse4DAdaptor'),
    dict(
        type='Collect',
        keys=['img', 'timestamp', 'projection_mat', 'image_wh'],
        meta_keys=['T_global', 'T_global_inv', 'timestamp'])
]
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
data_basic_config = dict(
    type='NuScenes3DDetTrackDataset',
    data_root='data/nuscenes/',
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    modality=dict(
        use_lidar=False,
        use_camera=True,
        use_radar=False,
        use_map=False,
        use_external=False),
    version='v1.0-trainval')
data_aug_conf = dict(
    resize_lim=(0.4, 0.47),
    final_dim=(256, 704),
    bot_pct_lim=(0.0, 0.0),
    rot_lim=(-5.4, 5.4),
    H=900,
    W=1600,
    rand_flip=True,
    rot3d_range=[-0.3925, 0.3925])
data = dict(
    samples_per_gpu=6,
    workers_per_gpu=6,
    train=dict(
        type='NuScenes3DDetTrackDataset',
        data_root='data/nuscenes/',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        version='v1.0-trainval',
        ann_file='data/nuscenes_anno_pkls/nuscenes_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(type='ResizeCropFlipImage'),
            dict(type='MultiScaleDepthMapGenerator', downsample=[4, 8, 16]),
            dict(type='BBoxRotation'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(
                type='CircleObjectRangeFilter',
                class_dist_thred=[55, 55, 55, 55, 55, 55, 55, 55, 55, 55]),
            dict(
                type='InstanceNameFilter',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(type='NuScenesSparse4DAdaptor'),
            dict(
                type='Collect',
                keys=[
                    'img', 'timestamp', 'projection_mat', 'image_wh',
                    'gt_depth', 'focal', 'gt_bboxes_3d', 'gt_labels_3d'
                ],
                meta_keys=[
                    'T_global', 'T_global_inv', 'timestamp', 'instance_id'
                ])
        ],
        test_mode=False,
        data_aug_conf=dict(
            resize_lim=(0.4, 0.47),
            final_dim=(256, 704),
            bot_pct_lim=(0.0, 0.0),
            rot_lim=(-5.4, 5.4),
            H=900,
            W=1600,
            rand_flip=True,
            rot3d_range=[-0.3925, 0.3925]),
        with_seq_flag=True,
        sequences_split_num=2,
        keep_consistent_seq_aug=True),
    val=dict(
        type='NuScenes3DDetTrackDataset',
        data_root='data/nuscenes/',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        version='v1.0-trainval',
        ann_file='data/nuscenes_anno_pkls/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(type='ResizeCropFlipImage'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='NuScenesSparse4DAdaptor'),
            dict(
                type='Collect',
                keys=['img', 'timestamp', 'projection_mat', 'image_wh'],
                meta_keys=['T_global', 'T_global_inv', 'timestamp'])
        ],
        data_aug_conf=dict(
            resize_lim=(0.4, 0.47),
            final_dim=(256, 704),
            bot_pct_lim=(0.0, 0.0),
            rot_lim=(-5.4, 5.4),
            H=900,
            W=1600,
            rand_flip=True,
            rot3d_range=[-0.3925, 0.3925]),
        test_mode=True,
        tracking=True,
        tracking_threshold=0.2),
    test=dict(
        type='NuScenes3DDetTrackDataset',
        data_root='data/nuscenes/',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        version='v1.0-trainval',
        ann_file='data/nuscenes_anno_pkls/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(type='ResizeCropFlipImage'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='NuScenesSparse4DAdaptor'),
            dict(
                type='Collect',
                keys=['img', 'timestamp', 'projection_mat', 'image_wh'],
                meta_keys=['T_global', 'T_global_inv', 'timestamp'])
        ],
        data_aug_conf=dict(
            resize_lim=(0.4, 0.47),
            final_dim=(256, 704),
            bot_pct_lim=(0.0, 0.0),
            rot_lim=(-5.4, 5.4),
            H=900,
            W=1600,
            rand_flip=True,
            rot3d_range=[-0.3925, 0.3925]),
        test_mode=True,
        tracking=True,
        tracking_threshold=0.2))
optimizer = dict(
    type='AdamW',
    lr=0.0006,
    weight_decay=0.001,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.5))))
optimizer_config = dict(grad_clip=dict(max_norm=25, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
runner = dict(type='IterBasedRunner', max_iters=58600)
vis_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(type='Collect', keys=['img'], meta_keys=['timestamp', 'lidar2img'])
]
evaluation = dict(
    interval=11720,
    pipeline=[
        dict(type='LoadMultiViewImageFromFiles', to_float32=True),
        dict(
            type='Collect', keys=['img'], meta_keys=['timestamp', 'lidar2img'])
    ])
gpu_ids = range(0, 1)

2024-11-29 03:58:12,242 - mmdet - INFO - Set random seed to 0, deterministic: False
2024-11-29 03:58:12,771 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'ckpt/resnet50-19c8e357.pth'}
2024-11-29 03:58:13,212 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from ckpt/resnet50-19c8e357.pth 

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

img_neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

img_neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

img_neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.instance_bank.anchor - torch.Size([900, 11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.instance_bank.instance_feature - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.instance_bank.anchor_handler.fix_scale - torch.Size([1, 3]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.0.weight - torch.Size([128, 3]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.3.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.5.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.5.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.6.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.6.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.8.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.8.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.9.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.9.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.11.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.pos_fc.11.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.0.weight - torch.Size([32, 3]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.3.weight - torch.Size([32, 32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.3.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.5.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.5.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.6.weight - torch.Size([32, 32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.6.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.8.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.8.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.9.weight - torch.Size([32, 32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.9.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.11.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.size_fc.11.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.0.weight - torch.Size([32, 2]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.3.weight - torch.Size([32, 32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.3.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.5.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.5.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.6.weight - torch.Size([32, 32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.6.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.8.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.8.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.9.weight - torch.Size([32, 32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.9.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.11.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.yaw_fc.11.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.0.weight - torch.Size([64, 3]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.3.weight - torch.Size([64, 64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.5.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.5.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.6.weight - torch.Size([64, 64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.6.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.8.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.8.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.9.weight - torch.Size([64, 64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.9.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.11.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.anchor_encoder.vel_fc.11.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.0.kps_generator.fix_scale - torch.Size([7, 3]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.0.kps_generator.learnable_fc.weight - torch.Size([18, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.0.kps_generator.learnable_fc.bias - torch.Size([18]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.0.output_proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.0.camera_encoder.0.weight - torch.Size([256, 12]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.0.camera_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.0.camera_encoder.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.0.camera_encoder.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.0.camera_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.0.camera_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.0.camera_encoder.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.0.camera_encoder.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.0.weights_fc.weight - torch.Size([416, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.0.weights_fc.bias - torch.Size([416]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.1.pre_norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.1.pre_norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.1.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.1.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.1.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.1.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.1.identity_fc.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.1.identity_fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.5.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.7.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.9.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.9.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.10.weight - torch.Size([11, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.10.bias - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.layers.11.scale - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.cls_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.cls_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.cls_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.cls_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.cls_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.cls_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.cls_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.cls_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.cls_layers.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.cls_layers.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.3.quality_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.quality_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.quality_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.quality_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.quality_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.quality_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.quality_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.quality_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.quality_layers.6.weight - torch.Size([2, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.3.quality_layers.6.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.4.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.4.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.4.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.4.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.5.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.5.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.5.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.5.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.6.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.6.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.7.kps_generator.fix_scale - torch.Size([7, 3]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.7.kps_generator.learnable_fc.weight - torch.Size([18, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.7.kps_generator.learnable_fc.bias - torch.Size([18]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.7.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.7.output_proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.7.camera_encoder.0.weight - torch.Size([256, 12]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.7.camera_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.7.camera_encoder.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.7.camera_encoder.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.7.camera_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.7.camera_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.7.camera_encoder.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.7.camera_encoder.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.7.weights_fc.weight - torch.Size([416, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.7.weights_fc.bias - torch.Size([416]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.8.pre_norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.8.pre_norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.8.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.8.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.8.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.8.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.8.identity_fc.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.8.identity_fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.9.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.9.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.5.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.7.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.9.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.9.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.10.weight - torch.Size([11, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.10.bias - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.layers.11.scale - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.cls_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.cls_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.cls_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.cls_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.cls_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.cls_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.cls_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.cls_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.cls_layers.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.cls_layers.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.10.quality_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.quality_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.quality_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.quality_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.quality_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.quality_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.quality_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.quality_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.quality_layers.6.weight - torch.Size([2, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.10.quality_layers.6.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.11.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.11.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.11.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.11.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.12.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.12.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.12.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.12.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.13.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.13.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.14.kps_generator.fix_scale - torch.Size([7, 3]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.14.kps_generator.learnable_fc.weight - torch.Size([18, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.14.kps_generator.learnable_fc.bias - torch.Size([18]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.14.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.14.output_proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.14.camera_encoder.0.weight - torch.Size([256, 12]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.14.camera_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.14.camera_encoder.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.14.camera_encoder.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.14.camera_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.14.camera_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.14.camera_encoder.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.14.camera_encoder.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.14.weights_fc.weight - torch.Size([416, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.14.weights_fc.bias - torch.Size([416]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.15.pre_norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.15.pre_norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.15.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.15.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.15.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.15.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.15.identity_fc.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.15.identity_fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.16.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.16.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.5.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.7.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.9.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.9.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.10.weight - torch.Size([11, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.10.bias - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.layers.11.scale - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.cls_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.cls_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.cls_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.cls_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.cls_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.cls_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.cls_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.cls_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.cls_layers.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.cls_layers.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.17.quality_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.quality_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.quality_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.quality_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.quality_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.quality_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.quality_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.quality_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.quality_layers.6.weight - torch.Size([2, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.17.quality_layers.6.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.18.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.18.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.18.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.18.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.19.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.19.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.19.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.19.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.20.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.20.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.21.kps_generator.fix_scale - torch.Size([7, 3]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.21.kps_generator.learnable_fc.weight - torch.Size([18, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.21.kps_generator.learnable_fc.bias - torch.Size([18]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.21.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.21.output_proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.21.camera_encoder.0.weight - torch.Size([256, 12]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.21.camera_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.21.camera_encoder.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.21.camera_encoder.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.21.camera_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.21.camera_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.21.camera_encoder.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.21.camera_encoder.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.21.weights_fc.weight - torch.Size([416, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.21.weights_fc.bias - torch.Size([416]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.22.pre_norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.22.pre_norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.22.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.22.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.22.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.22.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.22.identity_fc.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.22.identity_fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.23.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.23.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.5.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.7.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.9.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.9.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.10.weight - torch.Size([11, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.10.bias - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.layers.11.scale - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.cls_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.cls_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.cls_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.cls_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.cls_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.cls_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.cls_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.cls_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.cls_layers.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.cls_layers.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.24.quality_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.quality_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.quality_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.quality_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.quality_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.quality_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.quality_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.quality_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.quality_layers.6.weight - torch.Size([2, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.24.quality_layers.6.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.25.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.25.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.25.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.25.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.26.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.26.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.26.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.26.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.27.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.27.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.28.kps_generator.fix_scale - torch.Size([7, 3]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.28.kps_generator.learnable_fc.weight - torch.Size([18, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.28.kps_generator.learnable_fc.bias - torch.Size([18]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.28.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.28.output_proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.28.camera_encoder.0.weight - torch.Size([256, 12]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.28.camera_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.28.camera_encoder.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.28.camera_encoder.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.28.camera_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.28.camera_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.28.camera_encoder.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.28.camera_encoder.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.28.weights_fc.weight - torch.Size([416, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.28.weights_fc.bias - torch.Size([416]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.29.pre_norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.29.pre_norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.29.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.29.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.29.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.29.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.29.identity_fc.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.29.identity_fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.30.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.30.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.5.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.7.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.9.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.9.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.10.weight - torch.Size([11, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.10.bias - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.layers.11.scale - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.cls_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.cls_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.cls_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.cls_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.cls_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.cls_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.cls_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.cls_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.cls_layers.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.cls_layers.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.31.quality_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.quality_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.quality_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.quality_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.quality_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.quality_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.quality_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.quality_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.quality_layers.6.weight - torch.Size([2, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.31.quality_layers.6.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.32.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.32.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.32.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.32.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.33.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.33.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.33.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.33.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.34.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.34.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.35.kps_generator.fix_scale - torch.Size([7, 3]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.35.kps_generator.learnable_fc.weight - torch.Size([18, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.35.kps_generator.learnable_fc.bias - torch.Size([18]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.35.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.35.output_proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.35.camera_encoder.0.weight - torch.Size([256, 12]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.35.camera_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.35.camera_encoder.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.35.camera_encoder.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.35.camera_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.35.camera_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.35.camera_encoder.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.35.camera_encoder.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.35.weights_fc.weight - torch.Size([416, 256]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.35.weights_fc.bias - torch.Size([416]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.36.pre_norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.36.pre_norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.36.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.36.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.36.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.36.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.36.identity_fc.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.36.identity_fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.37.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.37.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.5.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.7.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.9.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.9.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.10.weight - torch.Size([11, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.10.bias - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.layers.11.scale - torch.Size([11]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.cls_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.cls_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.cls_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.cls_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.cls_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.cls_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.cls_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.cls_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.cls_layers.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.cls_layers.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in Sparse4DHead  

head.layers.38.quality_layers.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.quality_layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.quality_layers.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.quality_layers.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.quality_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.quality_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.quality_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.quality_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.quality_layers.6.weight - torch.Size([2, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.layers.38.quality_layers.6.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.fc_before.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of Sparse4D  

head.fc_after.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of Sparse4D  

depth_branch.depth_layers.0.weight - torch.Size([1, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of Sparse4D  

depth_branch.depth_layers.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of Sparse4D  

depth_branch.depth_layers.1.weight - torch.Size([1, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of Sparse4D  

depth_branch.depth_layers.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of Sparse4D  

depth_branch.depth_layers.2.weight - torch.Size([1, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of Sparse4D  

depth_branch.depth_layers.2.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of Sparse4D  
2024-11-29 03:58:13,419 - mmdet - INFO - Model:
Sparse4D(
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'ckpt/resnet50-19c8e357.pth'}
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (head): Sparse4DHead(
    (instance_bank): InstanceBank(
      (anchor_handler): SparseBox3DKeyPointsGenerator()
    )
    (anchor_encoder): SparseBox3DEncoder(
      (pos_fc): Sequential(
        (0): Linear(in_features=3, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (3): Linear(in_features=128, out_features=128, bias=True)
        (4): ReLU(inplace=True)
        (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (6): Linear(in_features=128, out_features=128, bias=True)
        (7): ReLU(inplace=True)
        (8): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (9): Linear(in_features=128, out_features=128, bias=True)
        (10): ReLU(inplace=True)
        (11): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (size_fc): Sequential(
        (0): Linear(in_features=3, out_features=32, bias=True)
        (1): ReLU(inplace=True)
        (2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (3): Linear(in_features=32, out_features=32, bias=True)
        (4): ReLU(inplace=True)
        (5): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (6): Linear(in_features=32, out_features=32, bias=True)
        (7): ReLU(inplace=True)
        (8): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (9): Linear(in_features=32, out_features=32, bias=True)
        (10): ReLU(inplace=True)
        (11): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (yaw_fc): Sequential(
        (0): Linear(in_features=2, out_features=32, bias=True)
        (1): ReLU(inplace=True)
        (2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (3): Linear(in_features=32, out_features=32, bias=True)
        (4): ReLU(inplace=True)
        (5): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (6): Linear(in_features=32, out_features=32, bias=True)
        (7): ReLU(inplace=True)
        (8): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (9): Linear(in_features=32, out_features=32, bias=True)
        (10): ReLU(inplace=True)
        (11): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (vel_fc): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): ReLU(inplace=True)
        (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (6): Linear(in_features=64, out_features=64, bias=True)
        (7): ReLU(inplace=True)
        (8): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (9): Linear(in_features=64, out_features=64, bias=True)
        (10): ReLU(inplace=True)
        (11): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (loss_cls): FocalLoss()
    (loss_reg): SparseBox3DLoss(
      (loss_box): L1Loss()
      (loss_cns): CrossEntropyLoss(avg_non_ignore=False)
      (loss_yns): GaussianFocalLoss()
    )
    (layers): ModuleList(
      (0): DeformableFeatureAggregation(
        (proj_drop): Dropout(p=0.0, inplace=False)
        (kps_generator): SparseBox3DKeyPointsGenerator(
          (learnable_fc): Linear(in_features=256, out_features=18, bias=True)
        )
        (output_proj): Linear(in_features=256, out_features=256, bias=True)
        (camera_encoder): Sequential(
          (0): Linear(in_features=12, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (weights_fc): Linear(in_features=256, out_features=416, bias=True)
      )
      (1): AsymmetricFFN(
        (activate): ReLU(inplace=True)
        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=512, out_features=1024, bias=True)
            (1): ReLU(inplace=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=1024, out_features=256, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): Identity()
        (identity_fc): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (3): SparseBox3DRefinementModule(
        (layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): Linear(in_features=256, out_features=256, bias=True)
          (6): ReLU(inplace=True)
          (7): Linear(in_features=256, out_features=256, bias=True)
          (8): ReLU(inplace=True)
          (9): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (10): Linear(in_features=256, out_features=11, bias=True)
          (11): Scale()
        )
        (cls_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (quality_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MultiheadAttention(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (proj_drop): Dropout(p=0.0, inplace=False)
        (dropout_layer): Dropout(p=0.1, inplace=False)
      )
      (5): MultiheadAttention(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (proj_drop): Dropout(p=0.0, inplace=False)
        (dropout_layer): Dropout(p=0.1, inplace=False)
      )
      (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (7): DeformableFeatureAggregation(
        (proj_drop): Dropout(p=0.0, inplace=False)
        (kps_generator): SparseBox3DKeyPointsGenerator(
          (learnable_fc): Linear(in_features=256, out_features=18, bias=True)
        )
        (output_proj): Linear(in_features=256, out_features=256, bias=True)
        (camera_encoder): Sequential(
          (0): Linear(in_features=12, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (weights_fc): Linear(in_features=256, out_features=416, bias=True)
      )
      (8): AsymmetricFFN(
        (activate): ReLU(inplace=True)
        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=512, out_features=1024, bias=True)
            (1): ReLU(inplace=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=1024, out_features=256, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): Identity()
        (identity_fc): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (10): SparseBox3DRefinementModule(
        (layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): Linear(in_features=256, out_features=256, bias=True)
          (6): ReLU(inplace=True)
          (7): Linear(in_features=256, out_features=256, bias=True)
          (8): ReLU(inplace=True)
          (9): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (10): Linear(in_features=256, out_features=11, bias=True)
          (11): Scale()
        )
        (cls_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (quality_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (11): MultiheadAttention(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (proj_drop): Dropout(p=0.0, inplace=False)
        (dropout_layer): Dropout(p=0.1, inplace=False)
      )
      (12): MultiheadAttention(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (proj_drop): Dropout(p=0.0, inplace=False)
        (dropout_layer): Dropout(p=0.1, inplace=False)
      )
      (13): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (14): DeformableFeatureAggregation(
        (proj_drop): Dropout(p=0.0, inplace=False)
        (kps_generator): SparseBox3DKeyPointsGenerator(
          (learnable_fc): Linear(in_features=256, out_features=18, bias=True)
        )
        (output_proj): Linear(in_features=256, out_features=256, bias=True)
        (camera_encoder): Sequential(
          (0): Linear(in_features=12, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (weights_fc): Linear(in_features=256, out_features=416, bias=True)
      )
      (15): AsymmetricFFN(
        (activate): ReLU(inplace=True)
        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=512, out_features=1024, bias=True)
            (1): ReLU(inplace=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=1024, out_features=256, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): Identity()
        (identity_fc): Linear(in_features=512, out_features=256, bias=True)
      )
      (16): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (17): SparseBox3DRefinementModule(
        (layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): Linear(in_features=256, out_features=256, bias=True)
          (6): ReLU(inplace=True)
          (7): Linear(in_features=256, out_features=256, bias=True)
          (8): ReLU(inplace=True)
          (9): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (10): Linear(in_features=256, out_features=11, bias=True)
          (11): Scale()
        )
        (cls_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (quality_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (18): MultiheadAttention(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (proj_drop): Dropout(p=0.0, inplace=False)
        (dropout_layer): Dropout(p=0.1, inplace=False)
      )
      (19): MultiheadAttention(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (proj_drop): Dropout(p=0.0, inplace=False)
        (dropout_layer): Dropout(p=0.1, inplace=False)
      )
      (20): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (21): DeformableFeatureAggregation(
        (proj_drop): Dropout(p=0.0, inplace=False)
        (kps_generator): SparseBox3DKeyPointsGenerator(
          (learnable_fc): Linear(in_features=256, out_features=18, bias=True)
        )
        (output_proj): Linear(in_features=256, out_features=256, bias=True)
        (camera_encoder): Sequential(
          (0): Linear(in_features=12, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (weights_fc): Linear(in_features=256, out_features=416, bias=True)
      )
      (22): AsymmetricFFN(
        (activate): ReLU(inplace=True)
        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=512, out_features=1024, bias=True)
            (1): ReLU(inplace=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=1024, out_features=256, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): Identity()
        (identity_fc): Linear(in_features=512, out_features=256, bias=True)
      )
      (23): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (24): SparseBox3DRefinementModule(
        (layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): Linear(in_features=256, out_features=256, bias=True)
          (6): ReLU(inplace=True)
          (7): Linear(in_features=256, out_features=256, bias=True)
          (8): ReLU(inplace=True)
          (9): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (10): Linear(in_features=256, out_features=11, bias=True)
          (11): Scale()
        )
        (cls_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (quality_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (25): MultiheadAttention(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (proj_drop): Dropout(p=0.0, inplace=False)
        (dropout_layer): Dropout(p=0.1, inplace=False)
      )
      (26): MultiheadAttention(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (proj_drop): Dropout(p=0.0, inplace=False)
        (dropout_layer): Dropout(p=0.1, inplace=False)
      )
      (27): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (28): DeformableFeatureAggregation(
        (proj_drop): Dropout(p=0.0, inplace=False)
        (kps_generator): SparseBox3DKeyPointsGenerator(
          (learnable_fc): Linear(in_features=256, out_features=18, bias=True)
        )
        (output_proj): Linear(in_features=256, out_features=256, bias=True)
        (camera_encoder): Sequential(
          (0): Linear(in_features=12, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (weights_fc): Linear(in_features=256, out_features=416, bias=True)
      )
      (29): AsymmetricFFN(
        (activate): ReLU(inplace=True)
        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=512, out_features=1024, bias=True)
            (1): ReLU(inplace=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=1024, out_features=256, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): Identity()
        (identity_fc): Linear(in_features=512, out_features=256, bias=True)
      )
      (30): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (31): SparseBox3DRefinementModule(
        (layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): Linear(in_features=256, out_features=256, bias=True)
          (6): ReLU(inplace=True)
          (7): Linear(in_features=256, out_features=256, bias=True)
          (8): ReLU(inplace=True)
          (9): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (10): Linear(in_features=256, out_features=11, bias=True)
          (11): Scale()
        )
        (cls_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (quality_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (32): MultiheadAttention(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (proj_drop): Dropout(p=0.0, inplace=False)
        (dropout_layer): Dropout(p=0.1, inplace=False)
      )
      (33): MultiheadAttention(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (proj_drop): Dropout(p=0.0, inplace=False)
        (dropout_layer): Dropout(p=0.1, inplace=False)
      )
      (34): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (35): DeformableFeatureAggregation(
        (proj_drop): Dropout(p=0.0, inplace=False)
        (kps_generator): SparseBox3DKeyPointsGenerator(
          (learnable_fc): Linear(in_features=256, out_features=18, bias=True)
        )
        (output_proj): Linear(in_features=256, out_features=256, bias=True)
        (camera_encoder): Sequential(
          (0): Linear(in_features=12, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (weights_fc): Linear(in_features=256, out_features=416, bias=True)
      )
      (36): AsymmetricFFN(
        (activate): ReLU(inplace=True)
        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=512, out_features=1024, bias=True)
            (1): ReLU(inplace=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=1024, out_features=256, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): Identity()
        (identity_fc): Linear(in_features=512, out_features=256, bias=True)
      )
      (37): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (38): SparseBox3DRefinementModule(
        (layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): Linear(in_features=256, out_features=256, bias=True)
          (6): ReLU(inplace=True)
          (7): Linear(in_features=256, out_features=256, bias=True)
          (8): ReLU(inplace=True)
          (9): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (10): Linear(in_features=256, out_features=11, bias=True)
          (11): Scale()
        )
        (cls_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (quality_layers): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): ReLU(inplace=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (fc_before): Linear(in_features=256, out_features=512, bias=False)
    (fc_after): Linear(in_features=512, out_features=256, bias=False)
  )
  (depth_branch): DenseDepthNet(
    (depth_layers): ModuleList(
      (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (grid_mask): GridMask()
)
2024-11-29 03:58:32,232 - mmdet - INFO - Start running, host: root@c6bf0db2fe3c, work_dir: /data/wangzhaohui/github/Sparse4D/work_dirs/sparse4dv3_temporal_r50_1x8_bs6_256x704
2024-11-29 03:58:32,233 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2024-11-29 03:58:32,233 - mmdet - INFO - workflow: [('train', 1)], max: 58600 iters
2024-11-29 03:58:32,242 - mmdet - INFO - Checkpoints will be saved to /data/wangzhaohui/github/Sparse4D/work_dirs/sparse4dv3_temporal_r50_1x8_bs6_256x704 by HardDiskBackend.
